{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4f55d3b",
   "metadata": {},
   "source": [
    "# Tutorial: Depth-from-Stereo Estimation with Foundation Stereo\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial demonstrates how to compute metric depth maps from Aria Gen2 stereo cameras using stereo rectification and the Foundation Stereo neural network. You will learn the complete pipeline from loading VRS data to visualizing 3D depth in Rerun.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Load stereo camera data from Aria Gen2 VRS files\n",
    "- Perform stereo rectification on fisheye images to align epipolar lines\n",
    "- Use Foundation Stereo for zero-shot disparity estimation\n",
    "- Convert disparity to metric depth using camera calibration\n",
    "- Visualize depth as interactive 3D point clouds with Rerun\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Stereo Rectification**: Process of transforming stereo images so epipolar lines are horizontal, simplifying stereo matching from 2D to 1D search\n",
    "- **Disparity**: Horizontal pixel displacement between corresponding points in left/right images\n",
    "- **Depth from Disparity**: `Z = (baseline Ã— focal_length) / disparity`\n",
    "- **Foundation Stereo**: Zero-shot stereo matching neural network that works across diverse scenes without fine-tuning\n",
    "\n",
    "**Prerequisites:**\n",
    "- CUDA-capable GPU (2-4GB VRAM)\n",
    "- Familiarity with Project Aria Tools (see Tutorial 1: VrsDataProvider Basics)\n",
    "- Understanding of camera calibration concepts (see Tutorial 2: Device Calibration)\n",
    "- Basic understanding of stereo geometry\n",
    "\n",
    "### âš ï¸ Important Notes\n",
    "- **Google Colab Users:**  \n",
    "  If you encounter a `ModuleNotFoundError: No module named 'rerun'` error after installing `rerun-sdk`, Colab may not recognize the new package until the runtime is restarted.  \n",
    "  **Fix:** Go to **Runtime â†’ Restart session and run all**.\n",
    "\n",
    "- **Visualization Issue:**  \n",
    "  If a Rerun visualization window does not appear, this may be due to a known caching issue. Simply re-run the visualization cell to resolve it."
   ]
  },
  {
   "cell_type": "code",
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# RUN AUTOMATED PIPELINE\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n!python run_colab.py",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# COLAB SETUP - Run this cell first\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# Clone repo\n!git clone https://github.com/jamesfortmeta/projectaria_gen2_depth_from_stereo\n%cd projectaria_gen2_depth_from_stereo\n\n# Install dependencies\n!pip install -q projectaria-tools[all]==2.0.0\n!pip install -q omegaconf flash-attn\n\n# Mount Google Drive\nfrom google.colab import drive\nimport shutil\nfrom pathlib import Path\ndrive.mount('/content/drive')\n\n# Copy model weights & data from Drive\nprint(\"ðŸ“¦ Copying data from Google Drive...\")\n\nPath('./FoundationStereo/pretrained_models').mkdir(parents=True, exist_ok=True)\nPath('./dataset').mkdir(exist_ok=True)\n\nshutil.copytree(\n    '/content/drive/MyDrive/aria_gen2_data/23-51-11',\n    './FoundationStereo/pretrained_models/23-51-11',\n    dirs_exist_ok=True\n)\n\nshutil.copytree(\n    '/content/drive/MyDrive/aria_gen2_data/cook_0',\n    './dataset/cook_0',\n    dirs_exist_ok=True\n)\n\nprint(\"âœ… Setup complete!\")\nprint(f\"   Model: {Path('./FoundationStereo/pretrained_models/23-51-11/model_best_bp2.pth').exists()}\")\nprint(f\"   VRS: {Path('./dataset/cook_0/video.vrs').exists()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "jjvlsf6jm9",
   "metadata": {},
   "source": [
    "## Setup Environment (Google Colab)\n",
    "\n",
    "If running on Google Colab, install `projectaria-tools`, Foundation Stereo dependencies, and download sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saztdg3nfjj",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "google_colab_env = 'google.colab' in str(get_ipython())\n",
    "\n",
    "if google_colab_env:\n",
    "    print(\"Running from Google Colab, installing dependencies and downloading sample data\")\n",
    "\n",
    "    # Install dependencies\n",
    "    !pip install projectaria-tools['all']==2.0.0\n",
    "    !pip install omegaconf\n",
    "\n",
    "    # Clone Foundation Stereo\n",
    "    !git clone https://github.com/NVlabs/FoundationStereo.git\n",
    "\n",
    "    # Download checkpoint (users need to obtain from NVIDIA)\n",
    "    # See: https://github.com/NVlabs/FoundationStereo for checkpoint download instructions\n",
    "    print(\"NOTE: You need to download Foundation Stereo checkpoints from NVIDIA.\")\n",
    "    print(\"Visit https://github.com/NVlabs/FoundationStereo for instructions.\")\n",
    "\n",
    "    # Download sample VRS\n",
    "    vrs_sample_path = \"./vrs_sample_data\"\n",
    "    vrs_url = \"https://www.projectaria.com/async/sample/download/?bucket=core&filename=aria_gen2_sample_data_1.vrs\"\n",
    "    vrs_filename = \"aria_gen2_sample_data_1.vrs\"\n",
    "    vrs_file_path = os.path.join(vrs_sample_path, vrs_filename)\n",
    "\n",
    "    command_list = [\n",
    "        f\"mkdir -p {vrs_sample_path}\",\n",
    "        f'curl -o {vrs_file_path} -C - -O -L \"{vrs_url}\"'\n",
    "    ]\n",
    "\n",
    "    print(f\"Downloading VRS sample data to {vrs_file_path}...\")\n",
    "    for command in command_list:\n",
    "        !$command\n",
    "\n",
    "    print(f\"Download complete! VRS file saved to: {vrs_file_path}\")\n",
    "\n",
    "    foundation_stereo_ckpt = \"./FoundationStereo/pretrained_models/23-51-11/model_best_bp2.pth\"\n",
    "\n",
    "    # Running this command to trigger early failure of importing ReRun\n",
    "    # Should be resolved by restarting the Colab session\n",
    "    import rerun as rr\n",
    "else:\n",
    "    # Local environment - update these paths\n",
    "    vrs_file_path = \"path/to/your/recording.vrs\"\n",
    "    foundation_stereo_ckpt = \"./FoundationStereo/pretrained_models/23-51-11/model_best_bp2.pth\"\n",
    "    print(\"Please update vrs_file_path to point to an Aria-Gen2 VRS file on your system\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6706a6",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, we'll import all required libraries and verify GPU availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a9109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Numerical and visualization imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# PyTorch for Foundation Stereo\n",
    "import torch\n",
    "\n",
    "# Project Aria Tools imports\n",
    "from projectaria_tools.core import data_provider, calibration\n",
    "from projectaria_tools.core.sensor_data import TimeDomain, TimeQueryOptions\n",
    "from projectaria_tools.core.stream_id import StreamId\n",
    "from projectaria_tools.core.sophus import SE3, SO3\n",
    "from projectaria_tools.core.image import InterpolationMethod\n",
    "\n",
    "# Rerun for 3D visualization\n",
    "import rerun as rr\n",
    "\n",
    "print(\"All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a49a98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foundation Stereo Setup\n",
    "from pathlib import Path\n",
    "\n",
    "# For Google Colab: clone repository\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    if not Path('./FoundationStereo').exists():\n",
    "        !git clone https://github.com/NVlabs/FoundationStereo.git\n",
    "    FOUNDATION_STEREO_PATH = './FoundationStereo'\n",
    "else:\n",
    "    # Local environment: check common locations or use environment variable\n",
    "    search_paths = [\n",
    "        os.environ.get('FOUNDATION_STEREO_PATH'),\n",
    "        Path.cwd() / 'FoundationStereo',\n",
    "        Path.home() / 'FoundationStereo',\n",
    "        Path.home() / 'projects' / 'FoundationStereo',\n",
    "        Path.home() / 'projects' / 'nebula_foundation_stereo_notebook' / 'FoundationStereo',\n",
    "    ]\n",
    "\n",
    "    FOUNDATION_STEREO_PATH = None\n",
    "    for path in search_paths:\n",
    "        if path and Path(path).exists() and (Path(path) / 'core').exists():\n",
    "            FOUNDATION_STEREO_PATH = str(path)\n",
    "            break\n",
    "\n",
    "    if FOUNDATION_STEREO_PATH is None:\n",
    "        raise FileNotFoundError(\n",
    "            \"FoundationStereo not found. Please either:\\n\"\n",
    "            \"  1. git clone https://github.com/NVlabs/FoundationStereo.git\\n\"\n",
    "            \"  2. Set FOUNDATION_STEREO_PATH environment variable\\n\"\n",
    "        )\n",
    "\n",
    "# Add to Python path\n",
    "sys.path.insert(0, FOUNDATION_STEREO_PATH)\n",
    "\n",
    "# Import Foundation Stereo components\n",
    "from omegaconf import OmegaConf\n",
    "from core.foundation_stereo import FoundationStereo\n",
    "from core.utils.utils import InputPadder\n",
    "\n",
    "# Stereo utilities from projectaria_tools\n",
    "from .stereo_utils import (\n",
    "    create_scanline_rectified_cameras,\n",
    "    fisheye_to_linear_calib,\n",
    "    rectify_stereo_pair,\n",
    "    compute_stereo_baseline,\n",
    "    disparity_to_depth,\n",
    "    get_rectified_camera_transform,\n",
    ")\n",
    "\n",
    "print(f\"Foundation Stereo loaded from: {FOUNDATION_STEREO_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a8aa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "print(\"System Information:\")\n",
    "print(f\"  PyTorch version: {torch.__version__}\")\n",
    "print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(\"\\nGPU ready for Foundation Stereo inference\")\n",
    "else:\n",
    "    print(\"\\nWARNING: No GPU detected. Foundation Stereo requires CUDA.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbfc167",
   "metadata": {},
   "source": [
    "## 2. Configure Paths and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b13be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# VRS file path - update for your local environment\n",
    "VRS_FILE_PATH = vrs_file_path if 'vrs_file_path' in dir() else \"path/to/your/recording.vrs\"\n",
    "\n",
    "# Foundation Stereo checkpoint (relative to FOUNDATION_STEREO_PATH)\n",
    "FOUNDATION_STEREO_CKPT = os.path.join(FOUNDATION_STEREO_PATH, \"pretrained_models/23-51-11/model_best_bp2.pth\")\n",
    "\n",
    "# Frame to process\n",
    "FRAME_INDEX = 100\n",
    "\n",
    "# Rectification parameters\n",
    "OUTPUT_WIDTH = 512\n",
    "OUTPUT_HEIGHT = 512\n",
    "FOCAL_SCALE = 1.25\n",
    "\n",
    "# Foundation Stereo parameters\n",
    "VALID_ITERS = 32\n",
    "\n",
    "# Depth parameters\n",
    "MIN_DISPARITY = 1.0\n",
    "MAX_DEPTH = 20.0\n",
    "\n",
    "print(f\"VRS file: {VRS_FILE_PATH}\")\n",
    "print(f\"Checkpoint: {FOUNDATION_STEREO_CKPT}\")\n",
    "print(f\"Output: {OUTPUT_WIDTH}x{OUTPUT_HEIGHT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b780877c",
   "metadata": {},
   "source": [
    "## 3. Load VRS Data and Extract Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295ddc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VRS file\n",
    "print(f\"Loading VRS file: {VRS_FILE_PATH}\")\n",
    "vrs_data_provider = data_provider.create_vrs_data_provider(VRS_FILE_PATH)\n",
    "assert vrs_data_provider is not None, f\"Failed to load VRS file\"\n",
    "\n",
    "device_calib = vrs_data_provider.get_device_calibration()\n",
    "\n",
    "# Identify stereo camera streams\n",
    "left_stream_id = vrs_data_provider.get_stream_id_from_label(\"slam-front-left\")\n",
    "right_stream_id = vrs_data_provider.get_stream_id_from_label(\"slam-front-right\")\n",
    "\n",
    "assert left_stream_id is not None, \"Could not find slam-front-left stream\"\n",
    "assert right_stream_id is not None, \"Could not find slam-front-right stream\"\n",
    "\n",
    "left_num_frames = vrs_data_provider.get_num_data(left_stream_id)\n",
    "right_num_frames = vrs_data_provider.get_num_data(right_stream_id)\n",
    "\n",
    "print(f\"VRS file loaded\")\n",
    "print(f\"  Left: {left_stream_id}, {left_num_frames} frames\")\n",
    "print(f\"  Right: {right_stream_id}, {right_num_frames} frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53c1e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get camera calibrations\n",
    "left_calib = device_calib.get_camera_calib(\"slam-front-left\")\n",
    "right_calib = device_calib.get_camera_calib(\"slam-front-right\")\n",
    "\n",
    "T_left_cam_device = left_calib.get_transform_device_camera().inverse()\n",
    "T_right_cam_device = right_calib.get_transform_device_camera().inverse()\n",
    "\n",
    "baseline = compute_stereo_baseline(T_left_cam_device, T_right_cam_device)\n",
    "\n",
    "print(\"Camera Calibration:\")\n",
    "print(f\"  Left: {left_calib.get_model_name()}, {left_calib.get_image_size()}\")\n",
    "print(f\"  Right: {right_calib.get_model_name()}, {right_calib.get_image_size()}\")\n",
    "print(f\"  Baseline: {baseline*1000:.1f} mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfe1d48",
   "metadata": {},
   "source": [
    "## 4. Load Stereo Frame Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e3f5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "left_data, left_record = vrs_data_provider.get_image_data_by_index(left_stream_id, FRAME_INDEX)\n",
    "right_data, right_record = vrs_data_provider.get_image_data_by_index(right_stream_id, FRAME_INDEX)\n",
    "\n",
    "left_image = left_data.to_numpy_array()\n",
    "right_image = right_data.to_numpy_array()\n",
    "timestamp_ns = left_record.capture_timestamp_ns\n",
    "\n",
    "print(f\"Frame {FRAME_INDEX} loaded\")\n",
    "print(f\"  Shape: {left_image.shape}\")\n",
    "print(f\"  Timestamp: {timestamp_ns} ns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb072c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize original fisheye images\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].imshow(left_image, cmap='gray')\n",
    "axes[0].set_title(f'Original Left (Fisheye) - Frame {FRAME_INDEX}')\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(right_image, cmap='gray')\n",
    "axes[1].set_title(f'Original Right (Fisheye) - Frame {FRAME_INDEX}')\n",
    "axes[1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b61a16",
   "metadata": {},
   "source": [
    "## 5. Stereo Rectification\n",
    "\n",
    "**Understanding Stereo Rectification:**\n",
    "\n",
    "Stereo rectification transforms the stereo pair to satisfy:\n",
    "1. **Horizontal epipolar lines** - corresponding points lie on the same row\n",
    "2. **No lens distortion** - fisheye distortion is removed  \n",
    "3. **Common image plane** - both cameras project onto aligned planes\n",
    "\n",
    "This simplifies stereo matching from 2D to 1D search (along each row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94c88ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear (pinhole) camera calibrations\n",
    "print(\"Creating rectified camera models...\")\n",
    "\n",
    "left_linear = fisheye_to_linear_calib(\n",
    "    left_calib, focal_scale=FOCAL_SCALE,\n",
    "    output_width=OUTPUT_WIDTH, output_height=OUTPUT_HEIGHT\n",
    ")\n",
    "right_linear = fisheye_to_linear_calib(\n",
    "    right_calib, focal_scale=FOCAL_SCALE,\n",
    "    output_width=OUTPUT_WIDTH, output_height=OUTPUT_HEIGHT\n",
    ")\n",
    "\n",
    "print(f\"Linear models created: {OUTPUT_WIDTH}x{OUTPUT_HEIGHT}\")\n",
    "print(f\"  Focal length: {left_linear.get_projection_params()[0]:.1f} px\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91b6a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute rectification rotations\n",
    "Rl_n, Rr_n = create_scanline_rectified_cameras(T_left_cam_device, T_right_cam_device)\n",
    "print(\"Rectification rotations computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd884d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply rectification\n",
    "start_time = time.time()\n",
    "left_rectified, right_rectified = rectify_stereo_pair(\n",
    "    left_image, right_image, left_calib, right_calib,\n",
    "    left_linear, right_linear, Rl_n, Rr_n,\n",
    "    interpolation=InterpolationMethod.BILINEAR\n",
    ")\n",
    "rectify_time = (time.time() - start_time) * 1000\n",
    "\n",
    "print(f\"Rectification complete in {rectify_time:.1f} ms\")\n",
    "print(f\"  Output shape: {left_rectified.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72292018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize rectified images\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].imshow(left_rectified, cmap='gray')\n",
    "axes[0].set_title('Rectified Left (Pinhole)')\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(right_rectified, cmap='gray')\n",
    "axes[1].set_title('Rectified Right (Pinhole)')\n",
    "axes[1].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd79154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify epipolar alignment\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 10))\n",
    "stacked = np.hstack([left_rectified, right_rectified])\n",
    "ax.imshow(stacked, cmap='gray')\n",
    "for y in range(0, OUTPUT_HEIGHT, 40):\n",
    "    ax.axhline(y, color='red', linewidth=1.0, alpha=1.0)\n",
    "ax.set_title('Epipolar Line Alignment\\n(Red lines show corresponding rows)')\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Rectification verified: epipolar lines are horizontal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9cf543",
   "metadata": {},
   "source": [
    "## 6. Foundation Stereo Inference\n",
    "\n",
    "Foundation Stereo is a zero-shot stereo matching neural network that computes disparity without requiring fine-tuning for new scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf5b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Foundation Stereo model\n",
    "print(f\"Loading Foundation Stereo...\")\n",
    "cfg_path = os.path.join(os.path.dirname(FOUNDATION_STEREO_CKPT), 'cfg.yaml')\n",
    "cfg = OmegaConf.load(cfg_path)\n",
    "if 'vit_size' not in cfg:\n",
    "    cfg['vit_size'] = 'vitl'\n",
    "cfg.valid_iters = VALID_ITERS\n",
    "\n",
    "model = FoundationStereo(cfg)\n",
    "ckpt = torch.load(FOUNDATION_STEREO_CKPT, map_location='cpu', weights_only=False)\n",
    "model.load_state_dict(ckpt['model'])\n",
    "model = model.cuda().eval()\n",
    "\n",
    "print(f\"Model loaded (epoch {ckpt['epoch']})\")\n",
    "print(f\"  ViT: {cfg.vit_size}, Iterations: {cfg.valid_iters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bc26d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input tensors\n",
    "if len(left_rectified.shape) == 2:\n",
    "    left_rgb = np.stack([left_rectified] * 3, axis=-1)\n",
    "    right_rgb = np.stack([right_rectified] * 3, axis=-1)\n",
    "else:\n",
    "    left_rgb = left_rectified\n",
    "    right_rgb = right_rectified\n",
    "\n",
    "left_tensor = torch.from_numpy(left_rgb).float().cuda().permute(2, 0, 1).unsqueeze(0)\n",
    "right_tensor = torch.from_numpy(right_rgb).float().cuda().permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "padder = InputPadder(left_tensor.shape, divis_by=32, force_square=False)\n",
    "left_padded, right_padded = padder.pad(left_tensor, right_tensor)\n",
    "\n",
    "print(f\"Input shape: {left_padded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43df2e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "print(\"Running Foundation Stereo...\")\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.cuda.amp.autocast(True):\n",
    "    with torch.no_grad():\n",
    "        disparity = model.forward(left_padded, right_padded, iters=cfg.valid_iters, test_mode=True)\n",
    "\n",
    "inference_time = (time.time() - start_time) * 1000\n",
    "disparity = padder.unpad(disparity.float())\n",
    "disparity_map = disparity.cpu().numpy().squeeze()\n",
    "\n",
    "print(f\"Inference complete in {inference_time:.1f} ms\")\n",
    "print(f\"  Disparity range: [{disparity_map.min():.2f}, {disparity_map.max():.2f}] px\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53da3a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize disparity\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "axes[0].imshow(left_rectified, cmap='gray')\n",
    "axes[0].set_title('Left Rectified')\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(right_rectified, cmap='gray')\n",
    "axes[1].set_title('Right Rectified')\n",
    "axes[1].axis('off')\n",
    "disp_vis = axes[2].imshow(disparity_map, cmap='magma')\n",
    "axes[2].set_title('Disparity Map')\n",
    "axes[2].axis('off')\n",
    "plt.colorbar(disp_vis, ax=axes[2], fraction=0.046)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e063e8bb",
   "metadata": {},
   "source": [
    "## 7. Convert Disparity to Depth\n",
    "\n",
    "We convert disparity to metric depth using: **`depth = (baseline Ã— focal_length) / disparity`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4899d958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get focal length\n",
    "focal_length = left_linear.get_projection_params()[0]\n",
    "print(f\"Baseline: {baseline:.4f} m ({baseline*1000:.1f} mm)\")\n",
    "print(f\"Focal length: {focal_length:.2f} px\")\n",
    "print(f\"Formula: depth = ({baseline:.4f} Ã— {focal_length:.2f}) / disparity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b81007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to depth\n",
    "depth_map = disparity_to_depth(\n",
    "    disparity_map, baseline=baseline, focal_length=focal_length,\n",
    "    min_disparity=MIN_DISPARITY, max_depth=MAX_DEPTH\n",
    ")\n",
    "\n",
    "valid_depth = depth_map[depth_map > 0]\n",
    "print(f\"\\nDepth map computed\")\n",
    "print(f\"  Valid pixels: {len(valid_depth)/depth_map.size*100:.1f}%\")\n",
    "print(f\"  Depth range: [{valid_depth.min():.2f}, {valid_depth.max():.2f}] m\")\n",
    "print(f\"  Mean depth: {valid_depth.mean():.2f} m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a3b35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize depth\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes[0,0].imshow(left_rectified, cmap='gray')\n",
    "axes[0,0].set_title('Left Rectified')\n",
    "axes[0,0].axis('off')\n",
    "disp_vis = axes[0,1].imshow(disparity_map, cmap='magma')\n",
    "axes[0,1].set_title('Disparity (px)')\n",
    "axes[0,1].axis('off')\n",
    "plt.colorbar(disp_vis, ax=axes[0,1], fraction=0.046)\n",
    "depth_vis = axes[1,0].imshow(depth_map, cmap='turbo', vmin=0, vmax=10)\n",
    "axes[1,0].set_title('Depth (meters)')\n",
    "axes[1,0].axis('off')\n",
    "plt.colorbar(depth_vis, ax=axes[1,0], fraction=0.046)\n",
    "axes[1,1].hist(valid_depth, bins=50, color='steelblue', alpha=0.7)\n",
    "axes[1,1].set_xlabel('Depth (m)')\n",
    "axes[1,1].set_ylabel('Pixel Count')\n",
    "axes[1,1].set_title('Depth Distribution')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc6014b",
   "metadata": {},
   "source": [
    "## 8. 3D Visualization with Rerun\n",
    "\n",
    "Visualize the depth map as a 3D point cloud using Rerun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f2b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Rerun\n",
    "rr.init(\"aria_stereo_depth_tutorial\")\n",
    "rr.set_time_nanos(\"device_time\", timestamp_ns)\n",
    "print(\"Rerun initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c1caec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rectified camera transform\n",
    "# T_rect_device transforms: device -> rectified camera\n",
    "# For Rerun, we need camera-to-world (camera-to-device), so invert it\n",
    "T_rect_device = get_rectified_camera_transform(T_left_cam_device, Rl_n)\n",
    "T_device_rect = T_rect_device.inverse()  # camera -> device (for Rerun)\n",
    "\n",
    "fx, fy = left_linear.get_projection_params()[0], left_linear.get_projection_params()[1]\n",
    "cx, cy = left_linear.get_projection_params()[2], left_linear.get_projection_params()[3]\n",
    "\n",
    "# Log camera\n",
    "rr.log(\n",
    "    \"world/camera\",\n",
    "    rr.Pinhole(\n",
    "        resolution=[OUTPUT_WIDTH, OUTPUT_HEIGHT],\n",
    "        focal_length=[fx, fy],\n",
    "        principal_point=[cx, cy],\n",
    "    ),\n",
    "    static=True\n",
    ")\n",
    "rr.log(\n",
    "    \"world/camera\",\n",
    "    rr.Transform3D(\n",
    "        translation=T_device_rect.translation(),\n",
    "        mat3x3=T_device_rect.rotation().to_matrix(),\n",
    "    )\n",
    ")\n",
    "print(\"Camera logged to Rerun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c37ea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log stereo images and disparity to Rerun\n",
    "\n",
    "# Log left rectified image\n",
    "rr.log(\"images/left_rectified\", rr.Image(left_rectified))\n",
    "\n",
    "# Log right rectified image\n",
    "rr.log(\"images/right_rectified\", rr.Image(right_rectified))\n",
    "\n",
    "# Create colored disparity visualization using matplotlib colormap\n",
    "import matplotlib.cm as cm\n",
    "disp_normalized = (disparity_map - disparity_map.min()) / (disparity_map.max() - disparity_map.min() + 1e-6)\n",
    "disp_colored = (cm.magma(disp_normalized)[:, :, :3] * 255).astype(np.uint8)\n",
    "rr.log(\"images/disparity\", rr.Image(disp_colored))\n",
    "\n",
    "# Log depth as 3D point cloud with the left image as texture\n",
    "depth_mm = (depth_map * 1000).astype(np.uint16)\n",
    "rr.log(\n",
    "    \"world/camera\",\n",
    "    rr.DepthImage(\n",
    "        depth_mm,\n",
    "        meter=1000.0,\n",
    "        colormap=\"Turbo\",\n",
    "        point_fill_ratio=0.5,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Images logged to Rerun:\")\n",
    "print(\"  - images/left_rectified: Left rectified grayscale\")\n",
    "print(\"  - images/right_rectified: Right rectified grayscale\")\n",
    "print(\"  - images/disparity: Colored disparity map (magma)\")\n",
    "print(\"  - world/camera: 3D point cloud with depth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d746096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Rerun visualization with custom layout\n",
    "import rerun.blueprint as rrb\n",
    "\n",
    "# Create blueprint with large 3D view on top, images in a row at bottom\n",
    "blueprint = rrb.Vertical(\n",
    "    # Main 3D view (takes most of the space)\n",
    "    rrb.Spatial3DView(name=\"3D Depth\", origin=\"world\"),\n",
    "    # Row of image views at the bottom\n",
    "    rrb.Horizontal(\n",
    "        rrb.Spatial2DView(name=\"Left Rectified\", origin=\"images/left_rectified\"),\n",
    "        rrb.Spatial2DView(name=\"Right Rectified\", origin=\"images/right_rectified\"),\n",
    "        rrb.Spatial2DView(name=\"Disparity\", origin=\"images/disparity\"),\n",
    "    ),\n",
    "    row_shares=[3, 1],  # 3:1 ratio - 3D view is 3x taller than image row\n",
    ")\n",
    "\n",
    "# Send blueprint and display in notebook\n",
    "rr.send_blueprint(blueprint)\n",
    "rr.notebook_show()\n",
    "\n",
    "print(\"\\n3D visualization ready!\")\n",
    "print(\"\\nLayout:\")\n",
    "print(\"  - Top (large): 3D point cloud view\")\n",
    "print(\"  - Bottom row: Left | Right | Disparity\")"
   ]
  }
 ],
 "metadata": {
  "fileHeader": "",
  "fileUid": "fe13a016-5e18-4ba6-adca-4ab810b81609",
  "isAdHoc": false,
  "kernelspec": {
   "display_name": "foundation_stereo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}